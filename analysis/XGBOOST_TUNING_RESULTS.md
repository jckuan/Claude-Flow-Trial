# XGBoost Hyperparameter Tuning Results

**Date**: November 6, 2025  
**Primary Metric**: RMSPE (Root Mean Square Percentage Error)  
**Dataset**: Rossmann Store Sales (755,389 train, 43,065 validation)

---

## Results Summary

| Model | RMSPE ‚≠ê | RMSE | MAE | R¬≤ | Training Time |
|-------|---------|------|-----|-----|---------------|
| **XGBoost_DeepTrees** | **0.010757** | 90.33 | 37.49 | 0.9992 | 26.7s |
| XGBoost_Aggressive | 0.012505 | 94.33 | 47.14 | 0.9991 | 45.8s |
| XGBoost_Regularized | 0.017096 | 121.05 | 72.30 | 0.9985 | 25.5s |
| XGBoost_MoreTrees | 0.018565 | 132.25 | 81.86 | 0.9982 | 24.1s |
| XGBoost_Baseline | 0.020016 | 140.38 | 90.10 | 0.9980 | 14.0s |

---

## üèÜ Best Model: XGBoost_DeepTrees

### Hyperparameters
```python
{
    'n_estimators': 100,
    'max_depth': 10,
    'learning_rate': 0.05,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'min_child_weight': 1,
    'gamma': 0,
    'reg_alpha': 0,
    'reg_lambda': 1,
    'random_state': 42,
    'tree_method': 'hist'
}
```

### Performance Metrics
- **RMSPE**: 0.010757 (~1.08% average percentage error)
- **RMSE**: 90.33 (35% improvement over Random Forest's 169)
- **MAE**: 37.49 (Average error: $37.49 per prediction)
- **R¬≤**: 0.9992 (99.92% variance explained)
- **Training Time**: 26.7 seconds

### Key Insights
1. **Deeper trees (max_depth=10) significantly improved RMSPE** - from 0.020 to 0.011
2. **Lower learning rate (0.05) with deep trees** found better optima
3. **Trade-off**: Aggressive config had slightly worse RMSPE but better RMSE
4. **Regularization** helped but not as much as deeper trees

---

## Model Comparison vs Previous Baselines

| Model | RMSPE | RMSE | R¬≤ | Notes |
|-------|-------|------|-----|-------|
| Random Forest (200 trees) | ~0.024 | 169.00 | 0.9970 | Previous best |
| LightGBM (100 trees) | ~0.024 | 167.10 | 0.9971 | Similar to RF |
| **XGBoost_DeepTrees** | **0.010757** | **90.33** | **0.9992** | **New best!** |

### Improvements
- **RMSPE**: 55% reduction (0.024 ‚Üí 0.011)
- **RMSE**: 47% reduction (169 ‚Üí 90)
- **R¬≤**: 0.22% improvement (0.9970 ‚Üí 0.9992)

---

## Configuration Analysis

### XGBoost_DeepTrees (Best RMSPE)
- ‚úÖ Deep trees capture complex interactions
- ‚úÖ Lower learning rate prevents overfitting
- ‚úÖ Best balance of accuracy and training time

### XGBoost_Aggressive (Best RMSE)
- ‚úÖ More trees (300) and deeper (max_depth=8)
- ‚ö†Ô∏è Longer training (45.8s)
- ‚ö†Ô∏è Slightly higher RMSPE (0.0125 vs 0.0108)

### XGBoost_Regularized
- ‚úÖ Strong regularization prevents overfitting
- ‚ö†Ô∏è Conservative predictions
- üìä Good for production stability

---

## Next Steps

1. ‚úÖ **Step 2 Complete**: XGBoost hyperparameter tuning done
2. üîÑ **Step 3 In Progress**: Create ensemble model combining:
   - XGBoost_DeepTrees (weight: 0.4)
   - XGBoost_Aggressive (weight: 0.3)
   - Random Forest (weight: 0.2)
   - LightGBM (weight: 0.1)
3. ‚è≥ **Step 1 Pending**: Generate final submission with best model

---

## Files Generated

- `models/xgboost_baseline.pkl` - RMSPE: 0.020016
- `models/xgboost_deeptrees.pkl` - RMSPE: 0.010757 ‚≠ê
- `models/xgboost_moretrees.pkl` - RMSPE: 0.018565
- `models/xgboost_regularized.pkl` - RMSPE: 0.017096
- `models/xgboost_aggressive.pkl` - RMSPE: 0.012505
- `models/xgboost_tuning_results.csv` - Full results table

---

**Conclusion**: XGBoost_DeepTrees achieves **1.08% RMSPE**, representing a 55% improvement over previous models. The deep tree structure with moderate regularization captures complex temporal and store-level patterns in the sales data.
